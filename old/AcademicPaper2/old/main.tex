%%%%%%%% ICML 2020 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2020} with \usepackage[nohyperref]{icml2020} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2020}

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2020}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Submission and Formatting Instructions for ICML 2020}

\begin{document}

\twocolumn[
\icmltitle{Learning from Easy and Hard Examples}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2020
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
%\icmlauthor{Aeiau Zzzz}{equal,to}
%\icmlauthor{Bauiu C.~Yyyy}{equal,to,goo}
%\icmlauthor{Cieua Vvvvv}{goo}
%\icmlauthor{Iaesut Saoeu}{ed}
%\icmlauthor{Fiuea Rrrr}{to}
%\icmlauthor{Tateu %H.~Yasehe}{ed,to,goo}
%\icmlauthor{Aaoeu Iasoh}{goo}
%\icmlauthor{Buiui Eueu}{ed}
%\icmlauthor{Aeuia Zzzz}{ed}
%\icmlauthor{Bieea C.~Yyyy}{to,goo}
%\icmlauthor{Teoau Xxxx}{ed}
%\icmlauthor{Eee Pppp}{ed}
\end{icmlauthorlist}

\icmlaffiliation{to}{Department of Computation, University of Torontoland, Torontoland, Canada}
\icmlaffiliation{goo}{Googol ShallowMind, New London, Michigan, USA}
\icmlaffiliation{ed}{School of Computation, University of Edenborrow, Edenborrow, United Kingdom}

\icmlcorrespondingauthor{Cieua Vvvvv}{c.vvvvv@googol.com}
\icmlcorrespondingauthor{Eee Pppp}{ep@eden.co.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
In this paper we introduce an approach to confidence-rated prediction and demonstrate its application to some real-world problems
\end{abstract}

\newcommand{\cD}{{\cal D}}
\newcommand{\cT}{{\cal T}}
\newcommand{\cN}{{\cal N}}
\newcommand{\cA}{{\cal A}}
\newcommand{\cE}{{\cal E}}
\newcommand{\cF}{{\cal F}}
\newcommand{\cX}{{\cal X}}
\newcommand{\cY}{{\cal Y}}

\section{Motivation}
\label{sec:motivation}

The prevalent methodology of machine learning experiments is to collect a large data set, split it into training and testing, train a model on the training data and test it on the test data.  To compare different experiments one usually compares the corresponding test errors.

The main contribution of this paper is to devise {\em per-example} performance measures. The idea is to produce ensembles of models and use these ensembles to associate a level of confidence {\em to each example}.

A popular ensemble method is {\em Bagging} \cite{} in which each member of the $n$-member ensemble is trained on a bootstrap resample of the training examples.  Consider a binary classification problem and let $h_i(x) \in \{-1,1\}$ be the prediction generated by the ensemble member $h_i$.
The final prediction suggested in ~\cite{bagging} is to use the majority vote, i.e.
$\mbox{sign} (\sum_{i=1}^{n} h_i(x))$. As we show in this paper there are real world situation 
where it is better to output "?" or "abstain" if $|\sum_{i=1}^{n} h_i(x)| < k$. 

\iffalse
It is widely agreed that a prediction is more valuable when it comes with a confidence measure. (give citations for confidence rated predictions for NN, boosting, random forests).

Most of the work on confidence rated NN is based on adding a non-linear output layer which transforms the raw outputs into well calibrated probabilities. More formally, if $X=x$ is the input to the NN, $\hat{y}(x)$ is the output of the NN and $Y=y$ is the true label, we wish to find a calibration function $c$ such that $c(\hat{y}(x))$
is a distribution over the output space $\cY$ which is a good approximation of the conditional distribution 
$P(Y=y'|X=x)$. This confidence measure quantifies the uncertainty in the label $Y$ given the input $X$, which we call the {\em inherent uncertainty}. It does not capture uncertainty that is the result of the training data and training algorithm.

We call uncertainty that is a result of training the training process the {\em model uncertainty}. Adding to the notation above we define the training set as $T$ and the learning algorithm $A$. $A(T)$ is the trained neural network generated by algorithm $A$ on training set $T$. We assume a distribution $\cT$ over $T$ and a distribution $\cA$ over the learning algorithm $A$. By generating a training set according to $\cT$  and feeding it into a learning algorithm chosen according to $\cA$ results in a distribution $\cN$ over NN's.
The (true) model uncertainty is captured by $\sum_{n \in \cN} P(Y=y' | X=x,N=n)P(N=n)$. 
which takes into consideration the uncertainty in the training set, and the uncertainty in the algorithm.
\fi
This framework can be used in many scenarios. We list a few here:
\begin{itemize}
    \item {\bf IID} This is the classical setup where examples are drawn IID from a fixed but unknown distribution. The distribution $\cT$ is the product distribution over this fixed underlying distribution. This is the common setup for uniform convergence bounds for samples of size $n$. 
    Suppose we have an unlimited source of examples. In this case we can generate an infinite number of samples of size $n$ and compute the models uncertainty exactly.  A popular approximation for this imaginary setup is to use bootstrap: sample $n$ points with replacement from the original sample of size $n$. Bootstrap is the technique underlying Bagging~\cite{} and Random Forests~\cite{}.
    \item {\bf randomized starting point} One of the common techniques used to generate an ensemble of NN is to run the algorithm multiple times, each time with a different initial setting of the random weights.
    \item {\bf personalising} A common
\end{itemize}


Confidence is often equated with calibration, which is not exactly true because calibration is not dependent on the entire training set, while a proper confidence measure must take into account all available information, including the training sample. 
\iffalse
The classification \emph{margin} is often used to obtain a confidence measure for a specific example. This is the difference between the weight assigned to the correct label and the maximal weight assigned to any single incorrect label. Margin lies in the range $[-1,1]$ and an example is classified correctly if
and only if its margin is positive. A large positive margin is often interpreted as a confident correct classification.

We argue that the main drawback of the commonly used margin confidence measures is that they are based only on a single predictor. It neglects the fact that most of ML Theory is based on the notion of uniform convergence. Therefore, it is natural to seek a neighborhood of models (instead of a single one) that are close to the theoretically best classifier. Thus, a confidence measure that takes into account predictions from the neighborhood of models will be more informative and reliable.

A natural way to obtain the neighborhood of models that we are looking for is to apply boosting (or bagging) technique. That way, a set of models $\{f_{1}, \dots, f_{T}\}$ obtained after $T$ rounds of boosting will form the neighborhood. Then we ca use the weighted margin of these $T$ models as the proper confidence measure.
\fi

The rest of the paper is structured as follows. In
Section~\ref{sec:boosting_distribution_margin}, we formally describe the boosting distribution margin and
show how to use it to compute the confidence score. In
Section~\ref{sec:easy_ad_hard_examples}, we define the notion of easy and hard examples and show how the boosting distribution margin can be used to break down training examples into these two categories. In
Section~\ref{sec:algorithm}, we put forward a learning algorithm that makes use of the boosting distribution margin. In Section~\ref{sec:experiments}, we describe our experimental
setting and provide empirical results.

\section{Sufficient conditions}

Suppose $\{h_1,\ldots,h_n\}$ is an ensemble and Let $\{x_1,\ldots, x_m\}$ be a test set.

suppose we know that the error rate of each member is smaller than $\epsilon$.

\section{Boosting Distribution Margin}
\label{sec:boosting_distribution_margin}
\section{Easy and Hard Examples}
\label{sec:easy_ad_hard_examples}
\section{Algorithm}
\label{sec:algorithm}
\section{Experiments}
\label{sec:experiments}
\section{Conclusion}
\label{sec:conclusion}
% Acknowledgements should only appear in the accepted version.
\section*{Acknowledgements}



\bibliography{example_paper}
\bibliographystyle{icml2020}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DELETE THIS PART. DO NOT PLACE CONTENT AFTER THE REFERENCES!
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2020. Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
